{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "TEXT MODEL"
      ],
      "metadata": {
        "id": "DU3cRwAhNLQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6frjC0PhfkO1",
        "outputId": "f993bfb7-79c9-44ba-ddb2-8e755dd26114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdasIju3Bbdz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5545baa7-7a91-4e68-baad-a9422e310256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Word2Vec model already exists.\n",
            "Dataset columns: Index(['sentence', 'sentiment'], dtype='object')\n",
            "                                            sentence sentiment\n",
            "0  Protecting biodiversity is crucial for maintai...  positive\n",
            "1  Deforestation and habitat destruction threaten...  negative\n",
            "2  Conservation efforts should prioritize endange...  positive\n",
            "3  Climate change exacerbates existing threats to...  negative\n",
            "4  National parks and protected areas play a vita...  positive\n",
            "Encoded dataset:\n",
            "                                            sentence  sentiment\n",
            "0  Protecting biodiversity is crucial for maintai...          2\n",
            "1  Deforestation and habitat destruction threaten...          0\n",
            "2  Conservation efforts should prioritize endange...          2\n",
            "3  Climate change exacerbates existing threats to...          0\n",
            "4  National parks and protected areas play a vita...          2\n",
            "Epoch 1/10, Train Loss: 1.0969, Train Acc: 0.3104, Val Loss: 1.0901, Val Acc: 0.4062\n",
            "New best model! Saving...\n",
            "Epoch 2/10, Train Loss: 1.0886, Train Acc: 0.4250, Val Loss: 1.0867, Val Acc: 0.4062\n",
            "Epoch 3/10, Train Loss: 1.0835, Train Acc: 0.4354, Val Loss: 1.0843, Val Acc: 0.4062\n",
            "Epoch 4/10, Train Loss: 1.0850, Train Acc: 0.4146, Val Loss: 1.0847, Val Acc: 0.4062\n",
            "Epoch 5/10, Train Loss: 1.0848, Train Acc: 0.4146, Val Loss: 1.0849, Val Acc: 0.4062\n",
            "Epoch 6/10, Train Loss: 1.0920, Train Acc: 0.4042, Val Loss: 1.0857, Val Acc: 0.4062\n",
            "Epoch 7/10, Train Loss: 1.1019, Train Acc: 0.3729, Val Loss: 1.0851, Val Acc: 0.4062\n",
            "Epoch 8/10, Train Loss: 1.0839, Train Acc: 0.4146, Val Loss: 1.0852, Val Acc: 0.4062\n",
            "Epoch 9/10, Train Loss: 1.0819, Train Acc: 0.4250, Val Loss: 1.0849, Val Acc: 0.4062\n",
            "Epoch 10/10, Train Loss: 1.0809, Train Acc: 0.4250, Val Loss: 1.0854, Val Acc: 0.4062\n",
            "Test Accuracy: 0.4062\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00        15\n",
            "     neutral       0.00      0.00      0.00         9\n",
            "    positive       0.33      1.00      0.50        12\n",
            "\n",
            "    accuracy                           0.33        36\n",
            "   macro avg       0.11      0.33      0.17        36\n",
            "weighted avg       0.11      0.33      0.17        36\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-d0889982b67a>:248: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"/content/drive/My Drive/Model/Text_best_model.pt\"))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final model saved at /content/drive/My Drive/Model/TextClassifier_Final.pt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import download\n",
        "\n",
        "# Ensure necessary NLTK data is downloaded\n",
        "download('punkt')\n",
        "\n",
        "# Check for GPU\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "# Constants\n",
        "LR = 0.001\n",
        "BATCH_SIZE = 32  # Reduced batch size to prevent memory issues\n",
        "NUM_CLASSES = 3  # Example: negative, neutral, positive\n",
        "EPOCHS = 10\n",
        "HIDDEN_SIZE = 50\n",
        "SEQUENCE_LENGTH = 100\n",
        "VECTOR_SIZE = 300  # Embedding dimension\n",
        "WINDOW = 5         # Context window size\n",
        "MIN_COUNT = 1      # Minimum word frequency\n",
        "WORKERS = 4        # Number of threads\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Paths\n",
        "MODEL_PATH = '/content/drive/My Drive/Model/word2vec.model'\n",
        "DATA_PATH = '/content/drive/My Drive/Dataset/images_text3.csv'\n",
        "\n",
        "# Train Word2Vec model if not already saved\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(\"Training Word2Vec model...\")\n",
        "    sentences = [word_tokenize(\"This is a sample sentence for Word2Vec training.\")]\n",
        "    w2v_model = Word2Vec(\n",
        "        sentences=sentences,\n",
        "        vector_size=VECTOR_SIZE,\n",
        "        window=WINDOW,\n",
        "        min_count=MIN_COUNT,\n",
        "        workers=WORKERS\n",
        "    )\n",
        "    w2v_model.save(MODEL_PATH)\n",
        "    print(f\"Word2Vec model saved to {MODEL_PATH}\")\n",
        "else:\n",
        "    print(\"Word2Vec model already exists.\")\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "# Verify dataset structure\n",
        "print(\"Dataset columns:\", df.columns)\n",
        "print(df.head())\n",
        "\n",
        "# Define text and label columns\n",
        "sentence = 'sentence'  # Adjust column name based on your dataset\n",
        "sentiment = 'sentiment'  # Adjust column name based on your dataset\n",
        "\n",
        "# Add dummy labels if necessary\n",
        "if sentiment not in df.columns:\n",
        "    print(f\"'{sentiment}' column not found. Adding a dummy column for testing.\")\n",
        "    df[sentiment] = np.random.randint(0, NUM_CLASSES, size=len(df))\n",
        "\n",
        "# Ensure the required columns exist\n",
        "if sentence not in df.columns or sentiment not in df.columns:\n",
        "    raise ValueError(f\"Columns '{sentence}' or '{sentiment}' not found in dataset.\")\n",
        "\n",
        "# Load pre-trained Word2Vec model\n",
        "w2v_model = Word2Vec.load(MODEL_PATH)\n",
        "EMBEDDING_MATRIX = w2v_model.wv.vectors\n",
        "VOCAB_SIZE, EMBEDDING_SIZE = EMBEDDING_MATRIX.shape\n",
        "\n",
        "# Define LSTM-based TextClassifier\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        vocab_size, embedding_dim = EMBEDDING_MATRIX.shape\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(EMBEDDING_MATRIX), freeze=True)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.lstm = nn.LSTM(embedding_dim, HIDDEN_SIZE, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(HIDDEN_SIZE * 2, NUM_CLASSES)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        embedded = self.dropout(embedded)\n",
        "        output, _ = self.lstm(embedded)\n",
        "        last_hidden_state = torch.cat((output[:, -1, :HIDDEN_SIZE], output[:, 0, HIDDEN_SIZE:]), dim=1)\n",
        "        logits = self.fc(last_hidden_state)\n",
        "        return logits\n",
        "\n",
        "# Ensure the 'sentiment' column is encoded numerically\n",
        "LABEL_MAPPING = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "df['sentiment'] = df['sentiment'].map(LABEL_MAPPING)\n",
        "\n",
        "# Check for unmapped labels\n",
        "if df['sentiment'].isnull().any():\n",
        "    raise ValueError(\"Some labels could not be mapped. Please check your dataset.\")\n",
        "\n",
        "\n",
        "print(\"Encoded dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Define Dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, dataframe, sentence, sentiment, word2vec_model):\n",
        "        self.data = dataframe[sentence].tolist()\n",
        "        self.labels = dataframe[sentiment].tolist()\n",
        "        self.word2vec_model = word2vec_model\n",
        "        self.max_sequence_length = SEQUENCE_LENGTH\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sequence = self.data[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        # Tokenize the sentence\n",
        "        tokens = word_tokenize(sequence)\n",
        "\n",
        "        # Convert tokens to indices based on Word2Vec vocabulary\n",
        "        sequence_indices = [\n",
        "            self.word2vec_model.wv.key_to_index[word] for word in tokens if word in self.word2vec_model.wv\n",
        "        ]\n",
        "\n",
        "        # Pad or truncate the sequence to the fixed length\n",
        "        padded_sequence = (\n",
        "            sequence_indices[:self.max_sequence_length]\n",
        "            + [0] * (self.max_sequence_length - len(sequence_indices))\n",
        "        )[:self.max_sequence_length]\n",
        "\n",
        "        # Convert to tensors\n",
        "        padded_sequence = torch.tensor(padded_sequence, dtype=torch.long)\n",
        "        label = torch.tensor(label, dtype=torch.long)  # Fixed to work with numeric labels\n",
        "\n",
        "        return padded_sequence, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "# Prepare DataLoaders\n",
        "data = TextDataset(df, sentence, sentiment, w2v_model)\n",
        "indices = np.arange(len(data))\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_indices, test_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
        "train_data = Subset(data, train_indices)\n",
        "test_data = Subset(data, test_indices)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = TextClassifier().to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# Training function\n",
        "def train(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        # Unpack batch\n",
        "        text, label = batch\n",
        "        text, label = text.to(device), label.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(text)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output, label)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        running_loss += loss.item()\n",
        "        _, preds = torch.max(output, dim=1)\n",
        "        running_acc += (preds == label).sum().item() / len(label)\n",
        "\n",
        "    return running_loss / len(train_loader), running_acc / len(train_loader)\n",
        "\n",
        "\n",
        "# Validation function\n",
        "def val(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            # Unpack batch\n",
        "            text, label = batch\n",
        "            text, label = text.to(device), label.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(text)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(output, label)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(output, dim=1)\n",
        "            running_acc += (preds == label).sum().item() / len(label)\n",
        "\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(label.cpu().numpy())\n",
        "\n",
        "    return running_loss / len(test_loader), running_acc / len(test_loader), predictions, true_labels\n",
        "\n",
        "# Training loop\n",
        "best_val_acc = 0.0\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, DEVICE)\n",
        "    val_loss, val_acc, _, _ = val(model, test_loader, criterion, DEVICE)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
        "        f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\"\n",
        "    )\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        print(\"New best model! Saving...\")\n",
        "        torch.save(model.state_dict(), \"/content/drive/My Drive/Model/Text_best_model.pt\")\n",
        "        best_val_acc = val_acc\n",
        "\n",
        "# Final model evaluation\n",
        "model.load_state_dict(torch.load(\"/content/drive/My Drive/Model/Text_best_model.pt\"))\n",
        "_, test_acc, predictions, true_labels = val(model, test_loader, criterion, DEVICE)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "label_mapping = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}  # Update as needed\n",
        "true_labels_mapped = [label_mapping[label] for label in true_labels]\n",
        "predictions_mapped = [label_mapping[pred] for pred in predictions]\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(true_labels_mapped, predictions_mapped))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(true_labels_mapped, predictions_mapped, labels=list(label_mapping.values()))\n",
        "plt.figure(figsize=(8, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=list(label_mapping.values()), yticklabels=list(label_mapping.values()))\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Saving final model\n",
        "FINAL_MODEL_PATH = \"/content/drive/My Drive/Model/TextClassifier_Final.pt\"\n",
        "torch.save(model.state_dict(), FINAL_MODEL_PATH)\n",
        "print(f\"Final model saved at {FINAL_MODEL_PATH}\")\n",
        "\n",
        "# Example inference function\n",
        "def predict_text(text, model, w2v_model, label_mapping, device):\n",
        "    model.eval()\n",
        "    tokens = word_tokenize(text)\n",
        "    sequence_indices = [\n",
        "        w2v_model.wv.key_to_index[word] for word in tokens if word in w2v_model.wv\n",
        "    ]\n",
        "    padded_sequence = torch.tensor(sequence_indices + [0] * (SEQUENCE_LENGTH - len(sequence_indices)))[:SEQUENCE_LENGTH]\n",
        "    padded_sequence = padded_sequence.unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(padded_sequence)\n",
        "        _, predicted_label = torch.max(output, dim=1)\n",
        "    return label_mapping[predicted_label.item()]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of the inference function\n",
        "sample_text = \"Climate change exacerbates existing threats to biodiversity and ecosystem stability.\"\n",
        "predicted_class = predict_text(sample_text, model, w2v_model, label_mapping, DEVICE)\n",
        "print(f\"Predicted class for the text: {predicted_class}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neZeHwv-MvRA",
        "outputId": "8ac3f223-4942-465f-9eaf-aed1a7264883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class for the text: positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Below models have to be fixed**"
      ],
      "metadata": {
        "id": "W20v_cAWCCp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMAGE MODEL"
      ],
      "metadata": {
        "id": "OxBb7w8jNw6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "from torchviz import make_dot\n",
        "import pydot"
      ],
      "metadata": {
        "id": "sQqNxDCfNz-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SHAPE = 48\n",
        "NUM_CLASSES = 3\n",
        "BATCH_SIZE = 1024\n",
        "TEXT_COL = \"text_emo\"\n",
        "IMAGE_COL = 'local_path'\n",
        "LABEL_COL = 'label'\n",
        "LABELS_PATH = \"C:/Users/MourtadaHouari/Desktop/sentiment-analysis/data/processed/train_images.csv\"\n",
        "IMAGES_PATH_PREFIX = \"C:/Users/MourtadaHouari/Desktop/sentiment-analysis/data/processed\"\n",
        "LR = 0.001\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(DEVICE)\n",
        "torch.cuda.get_device_name(0)\n",
        "SAVE_DIR = \".\"\n",
        "EPOCHS=3"
      ],
      "metadata": {
        "id": "LccEYIEjN2x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "\n",
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "\n",
        "transform = transforms.Compose([\n",
        "                        transforms.Resize((IMG_SHAPE, IMG_SHAPE)),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                    ])\n",
        "\n",
        "\n",
        "def train(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    for batch_idx, (image, label) in enumerate(train_loader):\n",
        "        image, label = image.to(device), label.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(image)\n",
        "        loss = criterion(output, label)\n",
        "        acc = accuracy(output, label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        running_acc += acc.item()\n",
        "\n",
        "    return running_loss / len(train_loader), running_acc / len(train_loader)\n",
        "\n",
        "def predict(model, image, label=None):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image = image.to(DEVICE)\n",
        "        output = model(image)\n",
        "        result = torch.max(output, dim=1).indices[0].cpu().item()\n",
        "        print(\"predicted label:\", dic[result])\n",
        "        if label != None:\n",
        "            print(\"actual label:\", label)\n",
        "    return output\n",
        "\n",
        "def test(model, predict_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (image, label) in enumerate(predict_loader):\n",
        "            image, label = image.to(device), label.to(device)\n",
        "\n",
        "            output = model(image)\n",
        "            loss = criterion(output, label)\n",
        "            acc = accuracy(output, label)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            running_acc += acc.item()\n",
        "\n",
        "            _, preds = torch.max(output, dim=1)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(label.cpu().numpy())\n",
        "\n",
        "    avg_loss = running_loss / len(predict_loader)\n",
        "    avg_acc = running_acc / len(predict_loader)\n",
        "\n",
        "    return avg_loss, avg_acc, predictions, true_labels\n",
        "\n",
        "\n",
        "def val(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (image, label) in enumerate(test_loader):\n",
        "            image, label = image.to(device), label.to(device)\n",
        "\n",
        "            output = model(image)\n",
        "            loss = criterion(output, label)\n",
        "            acc = accuracy(output, label)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            running_acc += acc.item()\n",
        "\n",
        "    return running_loss / len(test_loader), running_acc / len(test_loader)\n",
        "\n",
        "def save_model(model, save_dir, filename):\n",
        "    torch.save(model.state_dict(), os.path.join(save_dir, filename))"
      ],
      "metadata": {
        "id": "Lh4Fx1aIN4_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ImageClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 5, padding=2)  # Increase filter size to 5x5\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 5, padding=2)  # Increase filter size to 5x5\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)  # Keep the filter size at 3x3\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)  # Keep the filter size at 3x3\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.pool4 = nn.MaxPool2d(2, 2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(256 * IMG_SHAPE//16 * IMG_SHAPE//16, 512)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, 3)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.pool3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.pool4(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu5(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "EUU4z6v4N85R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, csv_path):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.transform = transforms.Compose([\n",
        "                        transforms.Resize((IMG_SHAPE, IMG_SHAPE)),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                    ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.data.loc[idx, IMAGE_COL][1:]\n",
        "        image = Image.open(IMAGES_PATH_PREFIX+image_path).convert('RGB')\n",
        "        image = self.transform(image)\n",
        "\n",
        "        label = self.data.loc[idx, LABEL_COL]\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "4bc6zytLN_KG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = ImageDataset(LABELS_PATH)\n",
        "indices = np.arange(len(data))\n",
        "train_indices, test_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "train_data = Subset(data, train_indices)\n",
        "test_data = Subset(data, test_indices)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
        "\n",
        "model = ImageClassifier().to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "model"
      ],
      "metadata": {
        "id": "kJVF2BiKOBPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_acc = 0.0\n",
        "count = 0\n",
        "val_acc_list = []\n",
        "train_acc_list = []\n",
        "val_loss_list = []\n",
        "train_loss_list = []\n",
        "\n",
        "EPOCHS = 20\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, DEVICE)\n",
        "    val_loss, val_acc = val(model, test_loader, criterion, DEVICE)\n",
        "\n",
        "    train_acc_list.append(train_acc)\n",
        "    train_loss_list.append(train_loss)\n",
        "    val_acc_list.append(val_acc)\n",
        "    val_loss_list.append(val_loss)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch: {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
        "        f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\"\n",
        "    )\n",
        "\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        print(f\"New best model! Saving...\")\n",
        "        save_model(model, SAVE_DIR, \"attention_best_model.pt\")\n",
        "        best_val_acc = val_acc\n",
        "        print(f\"Best val acc: {best_val_acc:.4f}\")\n",
        "        count = 0\n",
        "    else:\n",
        "        count += 1\n",
        "        print(f\"Count: {count} of epochs have no improvement\")\n",
        "        if count == 5:\n",
        "            print(f\"Early stopping, best val acc: {best_val_acc:.4f}\")\n",
        "            break"
      ],
      "metadata": {
        "id": "1Fzd_uUrOD4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (axis1, axis2) = plt.subplots(nrows=1, ncols=2, figsize=(16,6))\n",
        "# summarize history for accuracy\n",
        "axis1.plot(train_acc_list, label='Train', linewidth=3)\n",
        "axis1.plot(val_acc_list, label='Validation', linewidth=3)\n",
        "axis1.set_title('Model accuracy', fontsize=16)\n",
        "axis1.set_ylabel('accuracy')\n",
        "axis1.set_xlabel('epoch')\n",
        "axis1.legend(loc='upper left')\n",
        "\n",
        "axis2.plot(train_loss_list, label='Train', linewidth=3)\n",
        "axis2.plot(val_loss_list, label='Validation', linewidth=3)\n",
        "axis2.set_title('Model loss', fontsize=16)\n",
        "axis2.set_ylabel('loss')\n",
        "axis2.set_xlabel('epoch')\n",
        "axis2.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qXU7UjCdOGvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "\n",
        "# Initialize the camera\n",
        "camera = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    # Read the current frame from the camera\n",
        "    ret, frame = camera.read()\n",
        "\n",
        "    # Display the frame\n",
        "    cv2.imshow(\"Camera\", frame)\n",
        "\n",
        "    # Check if the 'c' key is pressed\n",
        "    if cv2.waitKey(1) & 0xFF == ord('c'):\n",
        "        # Save the current frame as an image\n",
        "        cv2.imwrite(\"captured_image.jpg\", frame)\n",
        "        print(\"Image captured!\")\n",
        "\n",
        "        image = Image.open(\"captured_image.jpg\").convert('RGB')\n",
        "        image = transform(image).unsqueeze(0)\n",
        "        text = \"its fucked\"\n",
        "\n",
        "        predict(model, image, \"\")\n",
        "\n",
        "\n",
        "\n",
        "    # Check if the 'q' key is pressed to exit the loopq\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the camera and close the window\n",
        "camera.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "Msksb1MROJli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc, predictions, true_labels = test(model, test_loader, criterion, DEVICE)\n",
        "\n",
        "print(f\"Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "true_labels = [dic[i] for i in true_labels]\n",
        "predictions = [dic[i] for i in predictions]\n",
        "\n",
        "print(classification_report(true_labels,predictions))\n",
        "cm = confusion_matrix(true_labels,predictions)\n",
        "p = sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\" )\n",
        "plt.title('The Confusion Martrix of Image model')"
      ],
      "metadata": {
        "id": "6pI-FVUmOMJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MULTI-MODAL MODEL"
      ],
      "metadata": {
        "id": "HC5YKmxCOSUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from torchviz import make_dot"
      ],
      "metadata": {
        "id": "7Iwor92VOWVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# common vars\n",
        "LABEL_COL = 'label'\n",
        "CSV_DATA = \"images_text.csv\"\n",
        "NUM_CLASSES = 3\n",
        "LR = 0.001\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "EPOCHS=20\n",
        "BATCH_SIZE = 2024\n",
        "DF = pd.read_csv(CSV_DATA)\n",
        "\n",
        "\n",
        "# image vars\n",
        "IMG_SHAPE = 48\n",
        "IMAGE_COL = 'local_path'\n",
        "\n",
        "# text vars\n",
        "TEXT_COL = \"text_emo\"\n",
        "W2V_MODEL = Word2Vec.load(\"word2vec.model\")\n",
        "EMBEDDING_MATRIX = W2V_MODEL.wv.vectors\n",
        "SEQUECE_LENGTH = 100\n",
        "LSTM_HIDDEN_SIZE = 50\n",
        "VOCAB_SIZE, EMBEDDING_SIZE = W2V_MODEL.wv.vectors.shape\n",
        "HIDDEN_SIZE = 50"
      ],
      "metadata": {
        "id": "Sjcz23aVOYjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ImageClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.pool4 = nn.MaxPool2d(2, 2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(256 * IMG_SHAPE//16 * IMG_SHAPE//16, 512)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        # self.fc2 = nn.Linear(512, 3)\n",
        "        # self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.pool3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.pool4(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu5(x)\n",
        "        x = self.dropout2(x)\n",
        "        # x = self.fc2(x)\n",
        "        # x = self.softmax(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "0RiP_apcOa3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        vocab_size, embedding_dim = EMBEDDING_MATRIX.shape\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(EMBEDDING_MATRIX), freeze=True)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.lstm1 = nn.LSTM(embedding_dim, HIDDEN_SIZE, batch_first=True, bidirectional=True)\n",
        "        # self.fc = nn.Linear(HIDDEN_SIZE * 2, NUM_CLASSES) # *2bidirectional\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        embedded = self.dropout(embedded)\n",
        "        output1, _ = self.lstm1(embedded)\n",
        "        last_hidden_state = torch.cat((output1[:, -1, :HIDDEN_SIZE], output1[:, 0, HIDDEN_SIZE:]), dim=1) # output1[:, -1, :]\n",
        "        # logits = self.fc(last_hidden_state)\n",
        "        return last_hidden_state"
      ],
      "metadata": {
        "id": "ZlhacBOMOdoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# class MultimodalClassifier(nn.Module):\n",
        "#     def __init__(self, text_model, image_model):\n",
        "#         super(MultimodalClassifier, self).__init__()\n",
        "#         self.text_model = text_model\n",
        "#         self.image_model = image_model\n",
        "#         self.attention_text = nn.Linear(NUM_CLASSES, NUM_CLASSES)\n",
        "#         self.attention_image = nn.Linear(NUM_CLASSES, NUM_CLASSES)\n",
        "#         self.fc = nn.Linear(NUM_CLASSES * 2, NUM_CLASSES)\n",
        "\n",
        "#     def forward(self, text, image):\n",
        "#         text_out = self.text_model(text)\n",
        "#         image_out = self.image_model(image)\n",
        "\n",
        "#         # Calculate attention weights for text and image features\n",
        "#         attention_text_weights = torch.softmax(self.attention_text(text_out), dim=1)\n",
        "#         attention_image_weights = torch.softmax(self.attention_image(image_out), dim=1)\n",
        "\n",
        "#         # Apply attention weights to the text and image features\n",
        "#         attended_text = attention_text_weights * text_out\n",
        "#         attended_image = attention_image_weights * image_out\n",
        "\n",
        "#         # Concatenate the attended features\n",
        "#         fusion = torch.cat((attended_text, attended_image), dim=1)\n",
        "\n",
        "#         out = self.fc(fusion)\n",
        "#         return out\n",
        "class MultimodalClassifier(nn.Module):\n",
        "    def __init__(self, text_model, image_model):\n",
        "        super(MultimodalClassifier, self).__init__()\n",
        "        self.text_model = text_model\n",
        "        self.image_model = image_model\n",
        "        self.attention_weights = nn.Parameter(torch.Tensor(1, 1, 2))\n",
        "        self.fc = nn.Linear(HIDDEN_SIZE * 2 + 256, NUM_CLASSES)\n",
        "\n",
        "    def forward(self, text_input, image_input):\n",
        "        # Extract text features\n",
        "        text_features = self.text_model(text_input)  # Shape: [batch_size, HIDDEN_SIZE * 2]\n",
        "\n",
        "        # Extract image features\n",
        "        image_features = self.image_model(image_input)  # Shape: [batch_size, 256]\n",
        "\n",
        "        # Apply attention to text features\n",
        "        attention_scores = torch.matmul(text_features.unsqueeze(2), self.attention_weights)  # Shape: [batch_size, HIDDEN_SIZE * 2, 1]\n",
        "        attention_weights = torch.softmax(attention_scores, dim=1)  # Shape: [batch_size, HIDDEN_SIZE * 2, 1]\n",
        "        attended_text_features = (text_features * attention_weights.squeeze(2)).sum(dim=1)  # Shape: [batch_size, HIDDEN_SIZE * 2]\n",
        "\n",
        "        # Concatenate text features and image features\n",
        "        multimodal_features = torch.cat((attended_text_features, image_features), dim=1)  # Shape: [batch_size, HIDDEN_SIZE * 2 + 256]\n",
        "\n",
        "        # Classify the multimodal features\n",
        "        logits = self.fc(multimodal_features)  # Shape: [batch_size, NUM_CLASSES]\n",
        "        return logits"
      ],
      "metadata": {
        "id": "oKDWGC8QOfro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalClassifier(nn.Module):\n",
        "    def __init__(self, text_model, image_model):\n",
        "        super(MultimodalClassifier, self).__init__()\n",
        "        self.text_model = text_model\n",
        "        self.image_model = image_model\n",
        "        self.fc = nn.Linear(NUM_CLASSES * 2, NUM_CLASSES)\n",
        "\n",
        "    def forward(self, text, image):\n",
        "        text_out = self.text_model(text)\n",
        "        image_out = self.image_model(image)\n",
        "\n",
        "        # Element-wise multiplication of text and image features\n",
        "        fusion = text_out * image_out\n",
        "\n",
        "        # Flatten the fusion tensor\n",
        "        fusion = fusion.view(fusion.size(0), -1)\n",
        "\n",
        "        out = self.fc(fusion)\n",
        "        return out"
      ],
      "metadata": {
        "id": "39LdYFydOh5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, dataframe, word2vec_model):\n",
        "        self.transform = transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                    ])\n",
        "        self.dataframe = dataframe\n",
        "        self.word2vec_model = word2vec_model\n",
        "        self.preprocessed_sequences = self._preprocess_text()\n",
        "\n",
        "    def _preprocess_text(self):\n",
        "        preprocessed_sequences = []\n",
        "        for idx in range(len(self.dataframe)):\n",
        "            text = self.dataframe.loc[idx, TEXT_COL]\n",
        "            tokens = word_tokenize(text)\n",
        "            sequence_indices = [self.word2vec_model.wv.key_to_index[word] for word in tokens if word in self.word2vec_model.wv]\n",
        "            padded_sequence = torch.tensor(sequence_indices + [0] * (SEQUECE_LENGTH - len(sequence_indices)))[:SEQUECE_LENGTH]\n",
        "            preprocessed_sequences.append(padded_sequence)\n",
        "        return preprocessed_sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.dataframe.loc[idx, IMAGE_COL]\n",
        "        label = self.dataframe.loc[idx, LABEL_COL]\n",
        "\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        image = self.transform(image)\n",
        "\n",
        "        preprocessed_sequence = self.preprocessed_sequences[idx]\n",
        "\n",
        "        return preprocessed_sequence, image, label"
      ],
      "metadata": {
        "id": "J_g5Xb36Oj7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "\n",
        "\n",
        "def train(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    for batch_idx, (text, image, label) in enumerate(train_loader):\n",
        "        text, image, label = text.to(device), image.to(device), label.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(text, image)\n",
        "        loss = criterion(output, label)\n",
        "        acc = accuracy(output, label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        running_acc += acc.item()\n",
        "\n",
        "    return running_loss / len(train_loader), running_acc / len(train_loader)\n",
        "\n",
        "\n",
        "def val(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (text, image, label) in enumerate(test_loader):\n",
        "            text, image, label = text.to(device), image.to(device), label.to(device)\n",
        "\n",
        "            output = model(text, image)\n",
        "            loss = criterion(output, label)\n",
        "            acc = accuracy(output, label)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            running_acc += acc.item()\n",
        "\n",
        "    return running_loss / len(test_loader), running_acc / len(test_loader)\n",
        "\n",
        "\n",
        "def save_model(model, save_dir, filename):\n",
        "    torch.save(model.state_dict(), os.path.join(save_dir, filename))\n",
        "\n",
        "\n",
        "def test(model, predict_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (text, image, label) in enumerate(predict_loader):\n",
        "            text, image, label = text.to(device), image.to(device), label.to(device)\n",
        "\n",
        "            output = model(text, image)\n",
        "            loss = criterion(output, label)\n",
        "            acc = accuracy(output, label)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            running_acc += acc.item()\n",
        "\n",
        "            _, preds = torch.max(output, dim=1)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(label.cpu().numpy())\n",
        "\n",
        "    avg_loss = running_loss / len(predict_loader)\n",
        "    avg_acc = running_acc / len(predict_loader)\n",
        "\n",
        "    return avg_loss, avg_acc, predictions, true_labels\n",
        "\n",
        "dic = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "\n",
        "def predict(model, text, image, label=None, eval=True):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # text processissng\n",
        "    max_sequence_length = SEQUECE_LENGTH\n",
        "    sequence = text\n",
        "    label = label\n",
        "    tokens = word_tokenize(sequence)\n",
        "    sequence_indices = [W2V_MODEL.wv.key_to_index[word] for word in tokens if word in W2V_MODEL.wv]\n",
        "    padded_sequence = torch.tensor(sequence_indices + [0] * (max_sequence_length - len(sequence_indices)))[:max_sequence_length].unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image = image.to(DEVICE)\n",
        "        text = padded_sequence.to(DEVICE)\n",
        "\n",
        "\n",
        "        output = model(text, image)\n",
        "        result = torch.max(output, dim=1).indices[0].cpu().item()\n",
        "        print(\"predicted label:\", dic[result])\n",
        "        if label != None:\n",
        "            print(\"actual label:\", label)\n",
        "    return output"
      ],
      "metadata": {
        "id": "2nMRjc9ROmCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin'\n",
        "\n",
        "\n",
        "import multiprocessing as mp\n",
        "data = MultimodalDataset(DF, W2V_MODEL)\n",
        "\n",
        "indices = np.arange(len(data))\n",
        "train_indices, test_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
        "\n",
        "train_data = Subset(data, train_indices)\n",
        "test_data = Subset(data, test_indices)\n",
        "\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "# create data loaders for train and test sets\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
        "\n",
        "\n",
        "\n",
        "text_model = TextClassifier().to(DEVICE)\n",
        "image_model = ImageClassifier().to(DEVICE)\n",
        "\n",
        "model = MultimodalClassifier(text_model, image_model).to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "model\n",
        "\n",
        "preprocessed_sequence, image, label = next(iter(train_loader))\n",
        "output = model(preprocessed_sequence.to(DEVICE), image.to(DEVICE))\n",
        "\n",
        "make_dot(output)"
      ],
      "metadata": {
        "id": "_twSk7vROpio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing as mp\n",
        "data = MultimodalDataset(DF, W2V_MODEL)\n",
        "\n",
        "indices = np.arange(len(data))\n",
        "train_indices, test_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
        "\n",
        "train_data = Subset(data, train_indices)\n",
        "test_data = Subset(data, test_indices)\n",
        "\n",
        "BATCH_SIZE = 2048\n",
        "\n",
        "# create data loaders for train and test sets\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
        "\n",
        "\n",
        "\n",
        "text_model = TextClassifier().to(DEVICE)\n",
        "image_model = ImageClassifier().to(DEVICE)\n",
        "\n",
        "model = MultimodalClassifier(text_model, image_model).to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "model"
      ],
      "metadata": {
        "id": "i_6dF2ukOqsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "best_val_acc = 0.0\n",
        "count = 0\n",
        "val_acc_list = []\n",
        "train_acc_list = []\n",
        "val_loss_list = []\n",
        "train_loss_list = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, DEVICE)\n",
        "    val_loss, val_acc = val(model, test_loader, criterion, DEVICE)\n",
        "\n",
        "    train_acc_list.append(train_acc)\n",
        "    train_loss_list.append(train_loss)\n",
        "    val_acc_list.append(val_acc)\n",
        "    val_loss_list.append(val_loss)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch: {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
        "        f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\"\n",
        "    )\n",
        "\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        print(f\"New best model! Saving...\")\n",
        "        save_model(model, \"\", \"ATTENTION_FUSION_MULTI_MODEL_BEST_MODEL.pt\")\n",
        "        best_val_acc = val_acc\n",
        "        print(f\"Best val acc: {best_val_acc:.4f}\")\n",
        "        count = 0\n",
        "    else:\n",
        "        count += 1\n",
        "        print(f\"Count: {count} of epochs have no improvement\")\n",
        "        if count == 5:\n",
        "            print(f\"Early stopping, best val acc: {best_val_acc:.4f}\")\n",
        "            break"
      ],
      "metadata": {
        "id": "AGsGxgXAOtRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (axis1, axis2) = plt.subplots(nrows=1, ncols=2, figsize=(16,6))\n",
        "# summarize history for accuracy\n",
        "axis1.plot(train_acc_list, label='Train', linewidth=3)\n",
        "axis1.plot(val_acc_list, label='Validation', linewidth=3)\n",
        "axis1.set_title('Model accuracy', fontsize=16)\n",
        "axis1.set_ylabel('accuracy')\n",
        "axis1.set_xlabel('epoch')\n",
        "axis1.legend(loc='upper left')\n",
        "\n",
        "axis2.plot(train_loss_list, label='Train', linewidth=3)\n",
        "axis2.plot(val_loss_list, label='Validation', linewidth=3)\n",
        "axis2.set_title('Model loss', fontsize=16)\n",
        "axis2.set_ylabel('loss')\n",
        "axis2.set_xlabel('epoch')\n",
        "axis2.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ukVSG0i0Ov2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc, predictions, true_labels = test(model, test_loader, criterion, DEVICE)\n",
        "\n",
        "print(f\"Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "true_labels = [dic[i] for i in true_labels]\n",
        "predictions = [dic[i] for i in predictions]\n",
        "\n",
        "print(classification_report(true_labels,predictions))\n",
        "cm = confusion_matrix(true_labels,predictions, labels=[\"negative\", \"neutral\", \"positive\"])\n",
        "p = sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title('The Confusion Martrix of Text model')"
      ],
      "metadata": {
        "id": "_z2PlGPxOzkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "textmodel = TextClassifier().to(DEVICE)\n",
        "imagemodel = ImageClassifier().to(DEVICE)\n",
        "\n",
        "model = MultimodalClassifier(textmodel, imagemodel).to(DEVICE)\n",
        "\n",
        "model.load_state_dict(torch.load(\"ATTENTION_FUSION_MULTI_MODEL_BEST_MODEL.pt\"))\n",
        "\n",
        "transform = transforms.Compose([\n",
        "                        transforms.Resize((IMG_SHAPE, IMG_SHAPE)),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                    ])\n",
        "\n",
        "image = Image.open(\"./train_images/t4sa/782694539618349056.jpg\").convert('RGB')\n",
        "image = transform(image).unsqueeze(0)\n",
        "text = \"hhhh\"\n",
        "\n",
        "predict(model, text, image, \"positive\", False)"
      ],
      "metadata": {
        "id": "nU0qpJ4gO1gA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "\n",
        "# Initialize the camera\n",
        "camera = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    # Read the current frame from the camera\n",
        "    ret, frame = camera.read()\n",
        "\n",
        "    # Display the frame\n",
        "    cv2.imshow(\"Camera\", frame)\n",
        "\n",
        "    # Check if the 'c' key is pressed\n",
        "    if cv2.waitKey(1) & 0xFF == ord('c'):\n",
        "        # Save the current frame as an image\n",
        "        cv2.imwrite(\"captured_image.jpg\", frame)\n",
        "        print(\"Image captured!\")\n",
        "\n",
        "        image = Image.open(\"captured_image.jpg\").convert('RGB')\n",
        "        image = transform(image).unsqueeze(0)\n",
        "        text = \"its fucked\"\n",
        "\n",
        "        predict(model, text, image, \"positive\", False)\n",
        "\n",
        "\n",
        "\n",
        "    # Check if the 'q' key is pressed to exit the loopq\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the camera and close the window\n",
        "camera.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "whjPv6bzO3V-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}